{
 "metadata": {
  "name": "",
  "signature": "sha256:e30c252e090aca2c3d2d42fb21950af79e2dfd5e99c97780eaf180488fb0db2f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from make_corpus import get_corpus\n",
      "from get_test_df import get_tests\n",
      "from predict_language import make_prediction\n",
      "from bernoulli_classifier_maker import bernoulli_classifier\n",
      "from sklearn import metrics\n",
      "from sklearn.cross_validation import train_test_split\n",
      "import re\n",
      "import pandas as pd\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To demonstrate our classifier and how it is created.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To create the classifier:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "example_classifier = bernoulli_classifier()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then it can be pickled and saved to use for later."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "pickle_classiifer(classifier) - (But we don't need to do that becuase we already have made it!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we can give it a a file and it will try to determine the language of the code used inside of the file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "make_prediction(\"test/1\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "I predict the language is: clojure\n",
        "Most likely languages...\n",
        " [('clojure', 0.9894945599469107), ('python', 0.004294744812262926), ('javascript', 0.0018081532203282282)]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "which we can reference to the key given..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "key = pd.read_csv(\"python_challenge.csv\")\n",
      "key.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Filename</th>\n",
        "      <th>Language</th>\n",
        "      <th>Unnamed: 2</th>\n",
        "      <th>Unnamed: 3</th>\n",
        "      <th>Unnamed: 4</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1</td>\n",
        "      <td> clojure</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 2</td>\n",
        "      <td> clojure</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 3</td>\n",
        "      <td> clojure</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 4</td>\n",
        "      <td> clojure</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 5</td>\n",
        "      <td>  python</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "   Filename Language  Unnamed: 2  Unnamed: 3  Unnamed: 4\n",
        "0         1  clojure         NaN         NaN         NaN\n",
        "1         2  clojure         NaN         NaN         NaN\n",
        "2         3  clojure         NaN         NaN         NaN\n",
        "3         4  clojure         NaN         NaN         NaN\n",
        "4         5   python         NaN         NaN         NaN"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can see the first file was clojure!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What we used to create our classifier:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prog_langs = {\"clojure\": [\".clj\", \".cljs\", \".edn\", \".clojure\"],\n",
      "             \"haskell\" : [\".hs\", \".lhs\", \".ghc\"],\n",
      "             \"java\": [\".java\", \".jar\", \".class\"],\n",
      "             \"javascript\": [\".js\", \".javascript\"],\n",
      "             \"ocaml\": [\".ml\", \".ocaml\"],\n",
      "             \"perl\" : [\".pl\", \".pm\", \".t\", \".pod\", \".perl\"],\n",
      "             \"php\" : [\".php\", \".phtml\", \".php4\", \".php3\", \".php5\", \".phps\"],\n",
      "             \"python\" : [\".py\", \".pyw\", \".pyc\", \".pyo\", \".pyd\", \".python3\"],\n",
      "             \"ruby\" : [\"rb\", \".rbw\", \".jruby\"],\n",
      "             \"scala\" : [\".scala\"],\n",
      "             \"scheme\" : [\".scm\", \".ss\", \".racket\"]\n",
      "             }\n",
      "\n",
      "\n",
      "extensions = (\".clj\", \".cljs\", \".edn\", \".clojure\",\n",
      "             \".hs\", \".lhs\", \".ghc\",\".java\", \".jar\",\n",
      "             \".js\", \".javascript\", \".ml\", \".pl\",\n",
      "             \".pm\", \".t\", \".pod\", \".php\", \".phtml\",\n",
      "             \".php4\", \".php3\", \".php5\", \".phps\",\n",
      "             \".py\", \".pyw\", \".pyc\", \".pyo\", \".pyd\",\n",
      "             \".python3\", \"rb\", \".rbw\", \".scala\",\n",
      "             \".scm\", \".ss\", \".racket\", \".perl\", \".ocaml\",\n",
      "             \".jruby\")\n",
      "\n",
      "\n",
      "def get_corpus(dir_path):\n",
      "    \"\"\"Goes into a directory and all the directories below it. In each\n",
      "    directory it find all files of a certain type, and adds their text to\n",
      "    a list. Once it has searched through each directory it creates a dataframe\n",
      "    with all of the text from each file and what language it was. Then\n",
      "    it makes new columns. One is the Textblob of all the words in the text,\n",
      "    and another is the textblob.words.\"\"\"\n",
      "    text_list = []\n",
      "    ext_list = []\n",
      "    for subdir, dirs, files in os.walk(dir_path):\n",
      "        for name in files:\n",
      "            if name.endswith((extensions)):\n",
      "                ext_list.append(grab_extension(name))\n",
      "                with open(os.path.join(subdir, name),\\\n",
      "                errors=\"surrogateescape\") as f:\n",
      "                    text_list.append(f.read())\n",
      "    corpus_df = pd.DataFrame({\"Language\": ext_list, \"Text\" : text_list})\n",
      "    corpus_df[\"Textblob\"] = corpus_df.Text.apply((lambda x: TextBlob(x).words))\n",
      "    corpus_df[\"Textblob letters\"] = corpus_df.Text.apply((lambda x:\\\n",
      "                                                          TextBlob(x)))\n",
      "    return corpus_df\n",
      "\n",
      "\n",
      "def grab_extension(file):\n",
      "    \"\"\"Takes the extension and finds the language that is related to that\n",
      "    language.\"\"\"\n",
      "    for lang in prog_langs.keys():\n",
      "        for ext in prog_langs[lang]:\n",
      "            if file.endswith(ext):\n",
      "                return lang"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is used to grab the corpus of data, it goes through a directory and finds each file with the appropriate extension type reads in the data and inserts it into a dataframe."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we insert the features into the Dataframe."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_word(feature_list, dataframe):\n",
      "    for feature in feature_list:\n",
      "        def temp_fn(word_list):\n",
      "            for word in word_list:\n",
      "                if word == feature:\n",
      "                    return 1\n",
      "            return 0\n",
      "        dataframe[feature] = dataframe[\"Textblob\"].apply(temp_fn)\n",
      "\n",
      "\n",
      "def find_character(feature_list, dataframe):\n",
      "    for feature in feature_list:\n",
      "        def temp_fn(character_list):\n",
      "            for character in character_list:\n",
      "                if character == feature:\n",
      "                    return 1\n",
      "            return 0\n",
      "        dataframe[feature] = dataframe[\"Textblob letters\"].apply(temp_fn)\n",
      "\n",
      "\n",
      "def find_special_chars(special_chars, dataframe):\n",
      "    for feature in special_chars:\n",
      "        def temp_fn(text):\n",
      "            counter = len(re.findall(feature, text))\n",
      "            if counter > 0:\n",
      "                return 1\n",
      "            else:\n",
      "                return 0\n",
      "        dataframe[feature] = dataframe[\"Text\"].apply(temp_fn)\n",
      "\n",
      "\n",
      "def b_add_to_df(dataframe):\n",
      "    common_words =['Object', 'var', 'try', 'except', 'class', 'return',\n",
      "                   'self', 'public', 'static', 'Body', 'val', 'defn',\n",
      "                   'def', 'end', 'each', 'let', 'define', 'printf',\n",
      "                   \"function\", \"echo\", \"global\", \"foreach\", \"elif\",\n",
      "                   \"data\", \"move\", \"where\"]\n",
      "    common_chars = [\"!\", \"(\", \")\", \";\", \":\", \"$\", \"@\",\"[\", \"]\", \"|\", \"{\", \"}\"]\n",
      "    special_chars = [\"=>\", \"->\", \"php\"]\n",
      "    find_word(common_words, dataframe)\n",
      "    find_character(common_chars, dataframe)\n",
      "    find_special_chars(special_chars, dataframe)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We used Bernoulli so we just check for the presence of each feature, if it is present it returns 1, and if it isn't it returns 0, and a new column is added for each feature. We determined the features used by looking at examples of each coding language and trying to extract specific bits of code that each langauge seems to use that others do not."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can bring this all together by creating two classifiers one trained with all of our data, and one :"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This first classifier will be used later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = get_corpus(\"training/benchmarks/benchmarksgame/bench/\")\n",
      "b_add_to_df(df)\n",
      "first_classifier = BernoulliNB()\n",
      "first_classifier = first_classifier.fit(df.loc[0::,'Object':\"php\"], df.loc[0::,\"Language\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This classifier will be used to test the training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "second_classifier = BernoulliNB()\n",
      "attributes_train, attributes_test, class_train, class_test = train_test_split(df.loc[0::,'Object':\"php\"], df.loc[0::,\"Language\"], test_size=0.4, random_state=0)\n",
      "second_classifier = second_classifier.fit(attributes_train, class_train)\n",
      "second_prediction = second_classifier.predict(attributes_test)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(metrics.classification_report(class_test, second_prediction))\n",
      "print(metrics.confusion_matrix(class_test, second_prediction))\n",
      "print(metrics.f1_score(class_test, second_prediction))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "    clojure       0.94      0.94      0.94        16\n",
        "    haskell       0.81      1.00      0.89        17\n",
        "       java       1.00      1.00      1.00        26\n",
        " javascript       0.86      1.00      0.92        12\n",
        "      ocaml       0.87      0.87      0.87        15\n",
        "       perl       0.94      0.88      0.91        17\n",
        "        php       0.93      1.00      0.97        14\n",
        "     python       1.00      0.73      0.84        26\n",
        "       ruby       0.88      1.00      0.94        22\n",
        "      scala       1.00      0.95      0.98        21\n",
        "     scheme       0.93      0.88      0.90        16\n",
        "\n",
        "avg / total       0.93      0.93      0.92       202\n",
        "\n",
        "[[15  0  0  1  0  0  0  0  0  0  0]\n",
        " [ 0 17  0  0  0  0  0  0  0  0  0]\n",
        " [ 0  0 26  0  0  0  0  0  0  0  0]\n",
        " [ 0  0  0 12  0  0  0  0  0  0  0]\n",
        " [ 1  0  0  0 13  0  0  0  0  0  1]\n",
        " [ 0  1  0  0  1 15  0  0  0  0  0]\n",
        " [ 0  0  0  0  0  0 14  0  0  0  0]\n",
        " [ 0  1  0  1  0  1  1 19  3  0  0]\n",
        " [ 0  0  0  0  0  0  0  0 22  0  0]\n",
        " [ 0  1  0  0  0  0  0  0  0 20  0]\n",
        " [ 0  1  0  0  1  0  0  0  0  0 14]]\n",
        "0.924504723981\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can see the training data does well for predicting languages."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we can test it against the whole test set!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_df = get_tests(\"test/\")\n",
      "b_add_to_df(test_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_predictions = first_classifier.predict(test_df.loc[0::,'Object':\"php\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(metrics.classification_report(test_df.loc[0::,\"Language\"], test_predictions))\n",
      "print(metrics.confusion_matrix(test_df.loc[0::,\"Language\"], test_predictions))\n",
      "print(metrics.f1_score(test_df.loc[0::,\"Language\"], test_predictions))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "    clojure       1.00      0.75      0.86         4\n",
        "    haskell       0.75      1.00      0.86         3\n",
        "       java       1.00      1.00      1.00         2\n",
        " javascript       1.00      0.75      0.86         4\n",
        "      ocaml       1.00      1.00      1.00         2\n",
        "       perl       0.00      0.00      0.00         0\n",
        "        php       1.00      0.67      0.80         3\n",
        "     python       0.80      1.00      0.89         4\n",
        "       ruby       1.00      1.00      1.00         3\n",
        "      scala       1.00      1.00      1.00         2\n",
        "     scheme       1.00      1.00      1.00         3\n",
        "\n",
        "avg / total       0.95      0.90      0.91        30\n",
        "\n",
        "[[3 0 0 0 0 0 0 1 0 0 0]\n",
        " [0 3 0 0 0 0 0 0 0 0 0]\n",
        " [0 0 2 0 0 0 0 0 0 0 0]\n",
        " [0 1 0 3 0 0 0 0 0 0 0]\n",
        " [0 0 0 0 2 0 0 0 0 0 0]\n",
        " [0 0 0 0 0 0 0 0 0 0 0]\n",
        " [0 0 0 0 0 1 2 0 0 0 0]\n",
        " [0 0 0 0 0 0 0 4 0 0 0]\n",
        " [0 0 0 0 0 0 0 0 3 0 0]\n",
        " [0 0 0 0 0 0 0 0 0 2 0]\n",
        " [0 0 0 0 0 0 0 0 0 0 3]]\n",
        "0.912804232804\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/Bodandly/.pyenv/versions/Language_classifier/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1773: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
        "  'recall', 'true', average, warn_for)\n",
        "/Users/Bodandly/.pyenv/versions/Language_classifier/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1773: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
        "  'recall', 'true', average, warn_for)\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our classifier does pretty well at determining the language of the coding example. It still has trouble with PHP and javascript, but otherwise it does well. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}